# (c) 2025 Jonathan Brandt
# Licensed under the MIT License. See LICENSE file in the project root.

"""
Wiki API access functions
"""

import time
import json
import requests
import re
from datetime import datetime
from urllib.parse import quote

import mwparserfromhell
from itertools import islice
from cachetools import LRUCache
from bs4 import BeautifulSoup

from birddog.utility import (
    convert_utc_time,
    equal_text,
    fetch_url,
    form_text_item,
    format_date,
    get_text,
    lastmod,
    match_text,
    translate_page,
    is_linked,
    )

from birddog.logging import get_logger
_logger = get_logger()

# INITIALIZATION --------------------------------------------------------------

# global constants

ARCHIVE_BASE    = 'https://uk.wikisource.org'
WIKI_NAMESPACE  = 'Архів'
ARCHIVES        = None

# load static data resources

_archive_master_path = 'resources/archives_master.json'

with open(_archive_master_path, encoding="utf8") as f:
    data = json.load(f)
    ARCHIVES = data['archives']

def _inventory_subarchives(archives):
    subarchives = {}
    for arc in archives.values():
        for sub in arc.values():
            subarchives[sub['subarchive']['uk']] = sub['subarchive']
    return list(subarchives.values())

SUBARCHIVES = _inventory_subarchives(ARCHIVES)

# -------------------------------------------------------------------------------
# subarchive sniffer

def sniff_subarchives(archive):
    url = f'{ARCHIVE_BASE}/wiki/{archive}'
    result = {}
    soup = BeautifulSoup(requests.get(url).text, 'lxml')
    for div in soup.find_all('div', attrs = {'id': 'mw-content-text'}):
        for item in div.find_all('a'):
            if item.has_attr('title'):
                if item['title'].startswith(archive) or item['title'].replace(" ", "_").startswith(archive):
                    if 'redlink' not in item['href']:
                        parsed = item['title'].split('/')
                        if len(parsed) == 2 and parsed[1] != 'видання':
                            subarchive = parsed[1]
                            _logger.info(f'found subarchive: {parsed[0]}-{parsed[1]}')
                            result[subarchive] = {
                                'title': form_text_item(item['title']),
                                'archive': form_text_item(parsed[0]),
                                'subarchive': form_text_item(parsed[1]),
                                'description': form_text_item(item.text),
                                'link': item['href'],
                                }
    return result

def _comment_string():
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    return f"This file was generated by update_master_archive_list() on {timestamp}. Do not edit manually."

def update_master_archive_list():
    with open('resources/archives.json', encoding="utf8") as f:
        manifest = json.load(f)
    
    archives = {}
    for archive_name, archive in manifest["archives"].items():
        _logger.info(f"Searching {archive_name}")
        archives[archive_name] = sniff_subarchives(archive)
        translate_page(archives[archive_name])
        for sub, value in archives[archive_name].items():
            if sub == "Р":  # make sure Cyrillic Р maps to Latin R
                _logger.info("Mapping Cyrillic Р to Latin R")
                value["subarchive"]["en"] = "R"
            elif sub == "А": # make sure Cyrillic А maps to Latin A
                _logger.info("Mapping Cyrillic А to Latin A")
                value["subarchive"]["en"] = "A"

    for fond_name, fond_title in manifest["fonds"].items():
        fond_name = fond_name.split('-')
        if len(fond_name) == 1:
            fond_name.append('_')
        item = {
            "title": form_text_item(fond_title),
            "subarchive": form_text_item(fond_name[1])
        }
        translate_page(item)
        if not archives.get(fond_name[0]):
            archives[fond_name[0]] = {}
        archives[fond_name[0]][fond_name[1]] = item

    _logger.info(f"generate_master_archive_list: updating {_archive_master_path}")
    with open(_archive_master_path, "w") as file:
        file.write(json.dumps({ 
            'comment':  _comment_string(),
            'archives': archives
            }, indent=4))

def all_archives():
    return [[arc, sub['subarchive']['en']] for arc, archive in ARCHIVES.items() for sub in archive.values()]

def _select_subarchive(archive, subarchive):
    for key, value in archive.items():
        if subarchive is None or key == subarchive or value["subarchive"]["en"] == subarchive:
            return value

def find_archive(archive_tag, subarchive=None):
    archive = ARCHIVES[archive_tag]
    sub = _select_subarchive(archive, subarchive)
    return { "title": sub["title"], "subarchive": sub["subarchive"] }

# -------------------------------------------------------------------------------
# WikiSource archive scraping

# HTML page element processing
def form_element_text(element):
    """Given HTML element, return multilingual text item containing the inner text"""
    text = element.text.strip() if element is not None else None
    return form_text_item(text)

def read_page(url):
    """
    Extract archive information for given page using HTTP get.
    Return struct with page:
        title,
        description,
        table header,
        table contents,
        lastmod date,
        doc_link [only for case pages],
        thumb_link [only for case pages],
    """
    soup = BeautifulSoup(fetch_url(url), 'lxml')
    title = soup.find('span', attrs = {'class': 'mw-page-title-main'})
    desc = soup.find('span', attrs = {'id': 'header_section_text'})
    table = soup.find('table', attrs = {'class': 'wikitable'})
    children = []
    header = []
    if table:
        for tr_elem in table.find_all('tr'):
            if not header:
                for th_elem in tr_elem.find_all('th'):
                    header.append(form_element_text(th_elem))
            item = []
            for td_elem in tr_elem.find_all('td'):
                a_elem = td_elem.find('a')
                child_url = a_elem.get('href') if a_elem else None
                text = form_text_item(td_elem.text.strip())
                item.append({'text': text, 'link': child_url})
            if item:
                children.append(item)

    # check for document thumbnail
    doc_info = soup.find('figure', attrs = {'typeof': 'mw:File/Thumb'})
    doc_url = None
    thumb_url = None
    if doc_info:
        a_tag = doc_info.find('a')
        doc_url = a_tag.get('href')
        thumb_elem = a_tag.find('img')
        thumb_url = thumb_elem.get('src') if thumb_elem else None
    footer = soup.find('li', attrs={'id': 'footer-info-lastmod'})
    last_modified = lastmod(footer.text) if footer else None

    return {
        'title': form_element_text(title),
        'description': form_element_text(desc),
        'header': header,
        'children': children,
        'lastmod': last_modified,
        'link': url,
        'doc_link': doc_url,
        'thumb_link': thumb_url,
    }

def do_search(query_string, limit=10, offset=0):
    """
    Search archive site for matching entries, sorted on last modification date.
    For each hit, return dict with item with keys: title, link, and lastmod.
    """
    _logger.info(f'do_search({query_string}, limt={limit}, offset={offset})')
    query_string = quote(query_string, safe='', encoding=None, errors=None)
    url = f'{ARCHIVE_BASE}/w/index.php?limit={limit}&offset={offset}'
    url += f'&ns0=1&sort=last_edit_desc&search={query_string}'
    soup = BeautifulSoup(fetch_url(url), 'lxml')
    results = []
    for result in soup.find_all('li', attrs = {'class': 'mw-search-result'}):
        div = result.find('div', attrs = {'class': 'mw-search-result-heading'})
        data = result.find('div', attrs = {'class': 'mw-search-result-data'})
        data = data.text.strip()
        pos = data.find('-')
        data = format_date(data[(pos + 1):].strip())
        item = {
            'title': div.a['title'],
            'link': div.a['href'],
            'lastmod': data
        }
        results.append(item)
    return results

def report_page_changes(page):
    """
    Print a report of changes detected in check_page_changes().
    """
    if not isinstance(page, dict):
        page = page.page
    if 'refmod' not in page:
        _logger.info(f"No changes to report. Run check_page_changes first.")
        return
    _logger.info(
        f'Change report for {get_text(page["title"])},' +
        f' lastmod={page["lastmod"]}, refmod={page["refmod"]}')
    for key in ['title', 'description']:
        if page[key]['edit'] is not None:
            _logger.info(f'{key}: {page[key]["edit"]}')
    for child in page['children']:
        index = get_text(child[0]['text'])
        for i, item in enumerate(child):
            if 'edit' in item and item['edit'] is not None:
                _logger.info(f'{index}[{i}] ({item["edit"]}): {get_text(item["text"])}')

def check_page_changes(page, reference, report=False):
    """
    Compare a given page to a prior version of the same page and return any detected changes.
    """
    if not isinstance(page, dict):
        page = page.page
    if not isinstance(reference, dict):
        reference = reference.page
    page['refmod'] = reference['lastmod']
    for key in ['title', 'description']:
        changed = not equal_text(page[key], reference[key])
        page[key]['edit'] = 'changed' if changed else None
    ref_children = dict((get_text(c[0]['text']), c) for c in reference['children'])
    for child in page['children']:
        index = get_text(child[0]['text'])
        if index in ref_children:
            ref_child = ref_children[index]
            for item, ref_item in zip(child, ref_child):
                changed = not equal_text(item['text'], ref_item['text'])
                item['edit'] = 'changed' if changed else None
                if 'link' in item and is_linked(item['link']):
                    if 'link' in ref_item and is_linked(ref_item['link']):
                        item['link_edit'] = 'changed' if item['link'] != ref_item['link'] else None
                    else:
                        item['link_edit'] = 'added'
        else:
            for item in child:
                item['edit'] = 'added'
    if report:
        report_page_changes(page)

def _page_update_summary(archive, change_list):
    #assert isinstance(archive, Archive)
    archive_prefix = archive.url[:archive.url.rfind('/')]
    archive_prefix = archive_prefix.replace(ARCHIVE_BASE, '')
    archive_prefix = archive_prefix.replace('%3A', ':')
    # Form list of fonds belonging to this archive
    fond_list={get_text(c[0]['text']) for c in archive.children}
    result = {}
    for item in change_list:
        page_spec = item["title"].split('/')
        address = (archive.tag, archive.subarchive["en"])
        address += tuple(entry for entry in page_spec[1:])
        address = (address + ("",) * 3)[:5]
        fond = address[2]
        address = ','.join(address)
        mod_date = item["lastmod"]
        # Confirm that the item belongs to the selected archive
        if fond in fond_list and item["link"].startswith(archive_prefix):
            if address in result:
                result[address] = max(mod_date, result["address"])
            else:
                result[address] = mod_date
    return result

def check_page_updates(archive, cutoff_date):
    #assert isinstance(archive, Archive)
    change_list = []
    batch_size = 50
    offset = 0
    while True:
        _logger.info(f'check_page_updates: {archive.name}, {batch_size}, {offset}')
        changes = archive.latest_changes(limit=batch_size, offset=offset)
        change_list += changes
        if not changes or changes[-1]["lastmod"] < cutoff_date:
            break
        offset += batch_size
        batch_size *= 2 # search geometrically longer history
    change_list = [item for item in change_list if item["lastmod"] >= cutoff_date]
    _logger.info(f"check_page_updates, {len(change_list)}, changes found")
    return _page_update_summary(archive, change_list)

# -------------------------------------------------------------------------------
# Page revision history handling (using wiki API)

def wiki_title(page_title):
    return f'{WIKI_NAMESPACE}:{page_title}'

def history_url(page_title, limit=1):
    return ('https://uk.wikisource.org/w/api.php?action=query&format=json'
            '&prop=revisions&rvprop=ids|timestamp'
            f'&rvlimit={limit}&titles={quote(wiki_title(page_title))}')

def page_revision_url(page_title, revid):
    return ('https://uk.wikisource.org/w/index.php?'
            f'title={quote(wiki_title(page_title))}&oldid={revid}')

def get_page_history(page_title, limit=10):
    result = fetch_url(history_url(page_title, limit=limit), json=True)
    query = result.get('query')
    #_logger.info(f'get_page_history({page_title}, limit={limit}): result={query}')

    if not query:
        _logger.error(f'get_page_history({page_title}, limit={limit}): no result returned')
        return []
    pages = query.get('pages')
    if not pages:
        _logger.error(f'get_page_history({page_title}, limit={limit}): empty result returned')
        return []
    if '-1' in pages:
        _logger.error(f'get_page_history({page_title}, limit={limit}): unrecognized page name')
        return []
    # assume only one page is returned (in future, pass multiple to reduce api calls)
    for page in pages.values():
        history = [ {
            'revid': rev['revid'],
            'modified': convert_utc_time(rev['timestamp']),
            'link': page_revision_url(page_title, rev['revid'])
        } for rev in page.get('revisions') ]
        return history
    _logger.error(f'get_page_history({page_title}, limit={limit}): unexpected result returned')
    return []

def get_page_history_from_cutoff(page_title, cutoff_date):
    # search increasingly for cutoff date  because
    # api does not allow for paging through search results
    last_result_length = 0
    attempt = 50
    while True:
        result = get_page_history(page_title, limit=attempt)
        if not result:
            _logger.error(f'get_page_history({page_title}, cutoff_date={cutoff_date}): empty history')
            return []
        if len(result) == last_result_length:
            result[-1]['created'] = True
            return result # no more history to be had
        if result[-1]['modified'] <= cutoff_date:
            for index, item in enumerate(result):
                if item['modified'] <= cutoff_date:
                    return result[:(index+1)]
            return result
        # increase limit length and try again
        last_result_length = len(result)
        attempt *= 2


# -------------------------------------------------------------------------------
# History LRU

class HistoryLRU:
    def __init__(self, maxsize=500, reset_limit=60 * 60):
        self._reset_limit = reset_limit  # seconds
        self._timer_start = time.time()
        self._lru = LRUCache(maxsize=maxsize)

    def _flush_if_needed(self):
        if time.time() - self._timer_start >= self._reset_limit:
            _logger.info("HistoryLRU: flushing all entries")
            self._lru.clear()
            self._timer_start = time.time()

    def _filter_with_fallback(self, history, cutoff_date):
        split = next((i for i, h in enumerate(history) if h['modified'] <= cutoff_date), len(history))
        return history[:split + 1]

    def lookup(self, page_title, limit=10):
        self._flush_if_needed()
        try:
            history = self._lru[page_title]
            _logger.info(f"HistoryLRU.lookup({page_title}): cache hit")
            if len(history) >= limit:
                return history[:limit]
            _logger.info(f"HistoryLRU.lookup({page_title}): cache too short, refreshing")
        except KeyError:
            _logger.info(f"HistoryLRU.lookup({page_title}): cache miss")
        # Refresh
        history = get_page_history(page_title, limit=limit)
        self._lru[page_title] = history
        return history[:limit]

    def lookup_by_cutoff(self, page_title, cutoff_date):
        self._flush_if_needed()
        try:
            history = self._lru[page_title]
            _logger.info(f"HistoryLRU.lookup_by_cutoff({page_title}): cache hit")

            if history:
                oldest = history[-1]
                if oldest.get('created') or oldest['modified'] < cutoff_date:
                    # We have enough
                    return self._filter_with_fallback(history, cutoff_date)
                _logger.info(f"HistoryLRU.lookup_by_cutoff({page_title}): cache incomplete, refreshing")
        except KeyError:
            _logger.info(f"HistoryLRU.lookup_by_cutoff({page_title}): cache miss")

        # Refresh and filter
        history = get_page_history_from_cutoff(page_title, cutoff_date=cutoff_date)
        self._lru[page_title] = history
        return self._filter_with_fallback(history, cutoff_date)

# -------------------------------------------------------------------------------
# Document link extraction from wikitext

def _wiki_content_url(titles):
    batch_titles = '|'.join([quote(f'{WIKI_NAMESPACE}:{t}') for t in titles])
    return (f'{ARCHIVE_BASE}/w/api.php?'
            'action=query&format=json&prop=revisions&'
            'rvprop=content&rvslots=main&'
            f'titles={batch_titles}'
           )

def _extract_file_links(wikitext):
    wikicode = mwparserfromhell.parse(wikitext)
    file_links = []

    # 1. [[File:...]] wikilinks
    for link in wikicode.filter_wikilinks():
        title = str(link.title)
        if title.lower().startswith("file:"):
            file_links.append(title)

    # 2. Template param values
    for template in wikicode.filter_templates():
        for param in template.params:
            value_str = str(param.value)

            # (a) Extract wikilinks inside param value
            parsed_value = mwparserfromhell.parse(value_str)
            for link in parsed_value.filter_wikilinks():
                title = str(link.title)
                if title.lower().startswith("file:"):
                    file_links.append(title)

            # (b) Extract raw "File:..." patterns not wrapped in [[ ]]
            # Acceptable file name chars: letters, digits, spaces, punctuation
            raw_file_match = re.findall(r'\bFile:[^\|\}\n\r]+', value_str)
            file_links.extend(raw_file_match)

            # (c) Extract full file URLs (just in case)
            file_url_matches = re.findall(r'https?://uk\.wikisource\.org/wiki/File:([^\s|}]+)', value_str)
            for match in file_url_matches:
                file_links.append(f'File:{match.replace("_", " ")}')  # decode _

    return file_links

def _normalize_mediawiki_title(title):
    title = title.replace(' ', '_')         # Normalize space to underscore
    return title

def _file_link_to_url(link):
    if link.lower().startswith("file:"):
        filename = _normalize_mediawiki_title(link[5:])
        return f"/wiki/File:{filename}"
    else:
        return None

def _deduplicate_links(links):
    return list(dict.fromkeys(links))

def _chunked(iterable, size):
    """Yield successive chunks from iterable."""
    it = iter(iterable)
    while True:
        chunk = list(islice(it, size))
        if not chunk:
            break
        yield chunk

def batch_fetch_document_links(titles, map_to_url=True, chunk_size=20):
    if not isinstance(titles, (list, tuple)):
        titles = [titles]

    result = {}

    for chunk in _chunked(titles, chunk_size):
        data = fetch_url(_wiki_content_url(chunk), json=True)
        if not 'query' in data:
            _logger.error(f'batch_fetch_document_links returned:\n    {data}')
        for page in data['query']['pages'].values():
            title = page['title'].split(':', 1)[-1]  # strip 'Архів:' prefix
            try:
                wikitext = page['revisions'][0]['slots']['main']['*']
                links = _extract_file_links(wikitext)
                if map_to_url:
                    links = [_file_link_to_url(link) for link in links]
                result[title] = _deduplicate_links(links)
            except (KeyError, IndexError):
                result[title] = []

    return result
